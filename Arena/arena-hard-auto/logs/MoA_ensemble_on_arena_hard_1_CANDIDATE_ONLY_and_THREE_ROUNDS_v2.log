nohup: ignoring input
{'name': 'config of answer generation for arena-hard-v0.1', 'bench_name': 'arena-hard-v0.1', 'temperature': 0.0, 'max_tokens': 4096, 'num_choices': 1, 'model_list': ['MoA_Ensemble_Three_Rounds']}
Output to data/arena-hard-v0.1/model_answer/MoA_Ensemble_Three_Rounds.jsonl
4 number of existing answers
  0%|          | 0/496 [00:00<?, ?it/s]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  0%|          | 1/496 [05:18<43:45:44, 318.27s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  0%|          | 2/496 [11:49<49:35:32, 361.40s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4814 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  1%|          | 3/496 [18:38<52:26:55, 382.99s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  1%|          | 4/496 [24:44<51:24:25, 376.15s/it]Round 1/3 to collecting reference responses.
Retry in 1s..
Retry in 2s..
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  1%|          | 5/496 [30:03<48:31:37, 355.80s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5098 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
Retry in 1s..
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5481 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  1%|          | 6/496 [41:42<64:17:13, 472.31s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4541 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  1%|▏         | 7/496 [48:08<60:18:57, 444.04s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4854 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  2%|▏         | 8/496 [54:07<56:32:39, 417.13s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4558 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  2%|▏         | 9/496 [1:01:20<57:06:27, 422.15s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7647 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4239 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  2%|▏         | 10/496 [1:08:18<56:48:02, 420.75s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  2%|▏         | 11/496 [1:10:46<45:26:21, 337.28s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  2%|▏         | 12/496 [1:13:57<39:21:04, 292.69s/it]Round 1/3 to collecting reference responses.
Retry in 1s..
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  3%|▎         | 13/496 [1:20:29<43:18:23, 322.78s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
Retry in 2s..
Round 3/3 to collecting reference responses.
Retry in 1s..
  3%|▎         | 14/496 [1:26:14<44:07:25, 329.56s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
Retry in 1s..
  3%|▎         | 15/496 [1:31:51<44:19:15, 331.72s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
Retry in 2s..
Round 3/3 to collecting reference responses.
Retry in 1s..
Retry in 2s..
  3%|▎         | 16/496 [1:36:42<42:37:02, 319.63s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  3%|▎         | 17/496 [1:40:55<39:49:57, 299.37s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  4%|▎         | 18/496 [1:45:54<39:44:46, 299.34s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  4%|▍         | 19/496 [1:50:13<38:02:57, 287.16s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  4%|▍         | 20/496 [1:52:29<31:58:53, 241.88s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  4%|▍         | 21/496 [1:56:18<31:24:12, 238.01s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  4%|▍         | 22/496 [1:58:21<26:47:11, 203.44s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4742 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4317 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  5%|▍         | 23/496 [2:03:38<31:13:15, 237.62s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  5%|▍         | 24/496 [2:08:37<33:34:55, 256.14s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  5%|▌         | 25/496 [2:11:47<30:53:13, 236.08s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  5%|▌         | 26/496 [2:14:36<28:12:09, 216.02s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4420 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  5%|▌         | 27/496 [2:20:17<33:01:29, 253.50s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  6%|▌         | 28/496 [2:25:22<34:57:34, 268.92s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
Retry in 2s..
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4532 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  6%|▌         | 29/496 [2:31:26<38:34:54, 297.42s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4482 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
  6%|▌         | 30/496 [2:36:56<39:47:28, 307.40s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  6%|▋         | 31/496 [2:40:23<35:48:03, 277.17s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  6%|▋         | 32/496 [2:43:54<33:08:59, 257.20s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  7%|▋         | 33/496 [2:49:17<35:37:08, 276.95s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4314 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
Retry in 1s..
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4930 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  7%|▋         | 34/496 [2:57:34<44:02:01, 343.12s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  7%|▋         | 35/496 [3:02:51<42:56:50, 335.38s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
Round 3/3 to collecting reference responses.
  7%|▋         | 36/496 [3:07:34<40:49:39, 319.52s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4424 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4126 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  7%|▋         | 37/496 [3:15:35<46:54:50, 367.95s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  8%|▊         | 38/496 [3:20:13<43:23:09, 341.03s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4488 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  8%|▊         | 39/496 [3:25:21<42:01:41, 331.07s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  8%|▊         | 40/496 [3:29:49<39:31:40, 312.06s/it]Round 1/3 to collecting reference responses.
Retry in 1s..
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4152 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  8%|▊         | 41/496 [3:35:38<40:52:15, 323.37s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4514 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
  8%|▊         | 42/496 [3:41:35<42:01:56, 333.30s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4146 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
  9%|▊         | 43/496 [3:47:55<43:42:30, 347.35s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  9%|▉         | 44/496 [3:50:32<36:27:16, 290.35s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  9%|▉         | 45/496 [3:55:24<36:26:01, 290.82s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
  9%|▉         | 46/496 [3:59:14<34:03:00, 272.40s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4307 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
  9%|▉         | 47/496 [4:04:25<35:26:42, 284.19s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 10%|▉         | 48/496 [4:06:51<30:11:19, 242.59s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 10%|▉         | 49/496 [4:10:10<28:30:51, 229.65s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 10%|█         | 50/496 [4:14:39<29:53:28, 241.27s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 10%|█         | 51/496 [4:19:05<30:44:03, 248.64s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4455 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 10%|█         | 52/496 [4:25:09<34:56:09, 283.27s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4602 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4256 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 11%|█         | 53/496 [4:32:24<40:27:53, 328.84s/it]Round 1/3 to collecting reference responses.
Retry in 1s..
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 11%|█         | 54/496 [4:37:26<39:22:43, 320.73s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 11%|█         | 55/496 [4:40:20<33:54:30, 276.80s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 11%|█▏        | 56/496 [4:46:09<36:28:18, 298.41s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4846 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7031 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 11%|█▏        | 57/496 [4:52:26<39:16:07, 322.02s/it]Round 1/3 to collecting reference responses.
Retry in 1s..
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 12%|█▏        | 58/496 [4:57:02<37:30:52, 308.34s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4118 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
 12%|█▏        | 59/496 [5:00:57<34:45:24, 286.33s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 12%|█▏        | 60/496 [5:04:59<33:02:14, 272.79s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4131 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 12%|█▏        | 61/496 [5:10:25<34:54:00, 288.83s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 6916 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 1s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 2s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 4s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 8s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 16s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 32s..
Round 3/3 to collecting reference responses.
 12%|█▎        | 62/496 [5:24:12<54:18:33, 450.49s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5623 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
 13%|█▎        | 63/496 [5:31:33<53:48:28, 447.36s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 13%|█▎        | 64/496 [5:35:30<46:07:02, 384.31s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 13%|█▎        | 65/496 [5:40:09<42:13:31, 352.69s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
Retry in 1s..
 13%|█▎        | 66/496 [5:44:20<38:28:59, 322.18s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 6473 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 1s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 2s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 4s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 8s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 16s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 32s..
Round 3/3 to collecting reference responses.
 14%|█▎        | 67/496 [5:56:18<52:33:46, 441.09s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 14%|█▎        | 68/496 [5:59:06<42:41:32, 359.09s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 6990 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 6757 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 14%|█▍        | 69/496 [6:05:57<44:26:04, 374.62s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 14%|█▍        | 70/496 [6:11:17<42:24:06, 358.33s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4533 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 14%|█▍        | 71/496 [6:16:48<41:19:06, 349.99s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 15%|█▍        | 72/496 [6:20:47<37:18:01, 316.70s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 15%|█▍        | 73/496 [6:24:21<33:36:10, 285.98s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 15%|█▍        | 74/496 [6:28:42<32:38:15, 278.43s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 15%|█▌        | 75/496 [6:33:05<32:01:26, 273.84s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 15%|█▌        | 76/496 [6:37:44<32:07:13, 275.32s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4745 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 16%|█▌        | 77/496 [6:43:29<34:29:03, 296.29s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 16%|█▌        | 78/496 [6:48:04<33:39:09, 289.83s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 16%|█▌        | 79/496 [6:52:57<33:41:39, 290.89s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 16%|█▌        | 80/496 [6:55:04<27:55:10, 241.61s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5048 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 16%|█▋        | 81/496 [7:01:04<31:57:21, 277.21s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 17%|█▋        | 82/496 [7:05:17<31:03:34, 270.08s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4463 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4503 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 17%|█▋        | 83/496 [7:11:05<33:38:15, 293.21s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5552 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 17%|█▋        | 84/496 [7:16:14<34:06:55, 298.09s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 17%|█▋        | 85/496 [7:20:11<31:55:58, 279.70s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4740 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
 17%|█▋        | 86/496 [7:24:28<31:06:04, 273.09s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4768 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4252 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 18%|█▊        | 87/496 [7:29:53<32:47:08, 288.58s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 18%|█▊        | 88/496 [7:32:59<29:11:42, 257.60s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4989 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4189 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 18%|█▊        | 89/496 [7:40:06<34:53:16, 308.59s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 18%|█▊        | 90/496 [7:44:11<32:39:29, 289.58s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 18%|█▊        | 91/496 [7:47:09<28:47:24, 255.91s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 19%|█▊        | 92/496 [7:51:02<27:57:06, 249.08s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4897 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 19%|█▉        | 93/496 [7:56:49<31:10:23, 278.47s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 19%|█▉        | 94/496 [8:02:11<32:33:41, 291.60s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 19%|█▉        | 95/496 [8:06:33<31:29:48, 282.76s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 19%|█▉        | 96/496 [8:11:54<32:40:10, 294.03s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 20%|█▉        | 97/496 [8:16:47<32:33:50, 293.81s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 20%|█▉        | 98/496 [8:22:02<33:11:56, 300.29s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 20%|█▉        | 99/496 [8:26:44<32:29:05, 294.57s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 20%|██        | 100/496 [8:31:54<32:54:54, 299.23s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
Retry in 1s..
 20%|██        | 101/496 [8:35:34<30:13:35, 275.48s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
Retry in 1s..
Retry in 2s..
 21%|██        | 102/496 [8:39:03<27:58:41, 255.64s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 21%|██        | 103/496 [8:43:00<27:17:50, 250.05s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 21%|██        | 104/496 [8:48:16<29:23:02, 269.85s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 21%|██        | 105/496 [8:53:29<30:43:34, 282.90s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 21%|██▏       | 106/496 [8:57:21<28:58:56, 267.53s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 22%|██▏       | 107/496 [9:00:50<26:59:51, 249.85s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 22%|██▏       | 108/496 [9:05:00<26:57:30, 250.13s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
Retry in 1s..
 22%|██▏       | 109/496 [9:11:58<32:17:39, 300.41s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4431 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 22%|██▏       | 110/496 [9:16:48<31:52:05, 297.22s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 22%|██▏       | 111/496 [9:21:02<30:23:04, 284.12s/it]Round 1/3 to collecting reference responses.
Retry in 1s..
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 23%|██▎       | 112/496 [9:25:25<29:38:52, 277.95s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 23%|██▎       | 113/496 [9:29:31<28:33:46, 268.48s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 6224 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5812 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 23%|██▎       | 114/496 [9:36:56<34:04:56, 321.19s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4459 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 23%|██▎       | 115/496 [9:41:23<32:17:52, 305.18s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 23%|██▎       | 116/496 [9:45:39<30:39:09, 290.39s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 24%|██▎       | 117/496 [9:50:25<30:25:47, 289.04s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 24%|██▍       | 118/496 [9:53:40<27:21:56, 260.62s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4482 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4674 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 24%|██▍       | 119/496 [10:00:51<32:38:58, 311.77s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 24%|██▍       | 120/496 [10:05:10<30:54:46, 295.98s/it]Round 1/3 to collecting reference responses.
Retry in 1s..
Retry in 2s..
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 24%|██▍       | 121/496 [10:08:47<28:21:13, 272.19s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 25%|██▍       | 122/496 [10:12:24<26:34:02, 255.73s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
Round 3/3 to collecting reference responses.
 25%|██▍       | 123/496 [10:17:25<27:54:30, 269.36s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4114 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
 25%|██▌       | 124/496 [10:21:49<27:40:57, 267.90s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5032 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4707 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 25%|██▌       | 125/496 [10:28:18<31:19:40, 303.99s/it]Round 1/3 to collecting reference responses.
Retry in 1s..
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4508 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4520 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 25%|██▌       | 126/496 [10:34:39<33:38:02, 327.25s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 26%|██▌       | 127/496 [10:36:58<27:45:12, 270.77s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 26%|██▌       | 128/496 [10:39:59<24:55:08, 243.77s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 26%|██▌       | 129/496 [10:41:45<20:39:08, 202.59s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4481 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
 26%|██▌       | 130/496 [10:47:36<25:06:21, 246.94s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 8881 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7224 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Retry in 1s..
Retry in 2s..
Retry in 4s..
 26%|██▋       | 131/496 [11:00:28<41:00:37, 404.49s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 7780 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
Retry in 1s..
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 6667 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 27%|██▋       | 132/496 [11:10:26<46:46:01, 462.53s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 6307 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 6670 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 27%|██▋       | 133/496 [11:18:42<47:38:50, 472.54s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5228 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 27%|██▋       | 134/496 [11:25:50<46:10:54, 459.27s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4902 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 27%|██▋       | 135/496 [11:31:39<42:43:58, 426.15s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 27%|██▋       | 136/496 [11:35:36<36:56:49, 369.47s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 28%|██▊       | 137/496 [11:38:35<31:07:58, 312.20s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 28%|██▊       | 138/496 [11:42:45<29:11:38, 293.57s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 28%|██▊       | 139/496 [11:46:00<26:11:16, 264.08s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 28%|██▊       | 140/496 [11:51:06<27:20:31, 276.49s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4191 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
 28%|██▊       | 141/496 [11:57:19<30:08:15, 305.62s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4426 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 29%|██▊       | 142/496 [12:02:44<30:36:28, 311.27s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4534 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4644 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 29%|██▉       | 143/496 [12:08:15<31:06:57, 317.33s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 29%|██▉       | 144/496 [12:12:35<29:20:24, 300.07s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 29%|██▉       | 145/496 [12:14:55<24:35:08, 252.16s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4183 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
 29%|██▉       | 146/496 [12:19:15<24:43:53, 254.38s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 30%|██▉       | 147/496 [12:22:34<23:03:38, 237.88s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5505 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5486 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 30%|██▉       | 148/496 [12:28:45<26:50:26, 277.66s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 30%|███       | 149/496 [12:34:04<27:57:20, 290.03s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 30%|███       | 150/496 [12:38:14<26:44:10, 278.18s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 30%|███       | 151/496 [12:41:41<24:37:13, 256.91s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 31%|███       | 152/496 [12:43:06<19:36:33, 205.21s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5124 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 31%|███       | 153/496 [12:48:22<22:43:00, 238.43s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4639 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4223 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 31%|███       | 154/496 [12:53:28<24:34:26, 258.67s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4398 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4993 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 31%|███▏      | 155/496 [13:01:05<30:08:19, 318.18s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 31%|███▏      | 156/496 [13:04:36<27:00:44, 286.01s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4424 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 32%|███▏      | 157/496 [13:09:56<27:54:38, 296.40s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 32%|███▏      | 158/496 [13:13:11<24:58:19, 265.97s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 32%|███▏      | 159/496 [13:17:15<24:15:29, 259.14s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 32%|███▏      | 160/496 [13:22:23<25:34:00, 273.93s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5070 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5369 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 32%|███▏      | 161/496 [13:28:04<27:20:56, 293.90s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4741 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4923 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 33%|███▎      | 162/496 [13:34:08<29:14:17, 315.14s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4228 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
 33%|███▎      | 163/496 [13:39:45<29:44:27, 321.52s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 33%|███▎      | 164/496 [13:41:02<22:54:07, 248.34s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 33%|███▎      | 165/496 [13:47:12<26:11:38, 284.89s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
Retry in 2s..
Round 3/3 to collecting reference responses.
 33%|███▎      | 166/496 [13:53:00<27:49:45, 303.59s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 34%|███▎      | 167/496 [13:57:57<27:34:09, 301.67s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 34%|███▍      | 168/496 [14:01:07<24:26:43, 268.30s/it]Round 1/3 to collecting reference responses.
Retry in 1s..
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5620 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 6126 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 34%|███▍      | 169/496 [14:08:46<29:33:32, 325.42s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 34%|███▍      | 170/496 [14:11:20<24:48:35, 273.97s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 34%|███▍      | 171/496 [14:13:50<21:23:23, 236.93s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 35%|███▍      | 172/496 [14:16:45<19:39:04, 218.35s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 35%|███▍      | 173/496 [14:21:22<21:09:21, 235.79s/it]Round 1/3 to collecting reference responses.
Retry in 1s..
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4485 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5462 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 35%|███▌      | 174/496 [14:29:22<27:38:09, 308.97s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
Retry in 2s..
Retry in 4s..
Round 3/3 to collecting reference responses.
 35%|███▌      | 175/496 [14:37:37<32:31:39, 364.80s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 35%|███▌      | 176/496 [14:40:33<27:24:45, 308.39s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 36%|███▌      | 177/496 [14:44:42<25:44:50, 290.57s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4185 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 36%|███▌      | 178/496 [14:49:04<24:53:38, 281.82s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4801 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4914 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 36%|███▌      | 179/496 [14:57:27<30:39:42, 348.21s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4657 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 36%|███▋      | 180/496 [15:02:32<29:25:42, 335.26s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 36%|███▋      | 181/496 [15:06:34<26:53:26, 307.32s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4391 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 37%|███▋      | 182/496 [15:11:03<25:48:14, 295.84s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 37%|███▋      | 183/496 [15:13:41<22:07:38, 254.50s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 37%|███▋      | 184/496 [15:17:31<21:24:23, 247.00s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4677 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 37%|███▋      | 185/496 [15:22:58<23:25:30, 271.16s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4524 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
 38%|███▊      | 186/496 [15:28:32<24:57:19, 289.80s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Retry in 1s..
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 5154 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 38%|███▊      | 187/496 [15:33:21<24:52:30, 289.81s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
 38%|███▊      | 188/496 [15:37:24<23:34:34, 275.57s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4326 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 38%|███▊      | 189/496 [15:41:33<22:50:02, 267.76s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4192 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
Round 3/3 to collecting reference responses.
 38%|███▊      | 190/496 [15:45:52<22:32:11, 265.14s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 1s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 2s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 4s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 8s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 16s..
------------------------------------------
Model with Error: databricks/dbrx-instruct
{'error': {'message': 'axios status: 504: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center>nginx/1.27.0</center>\r\n</body>\r\n</html>\r\n', 'type': 'server_error', 'param': None, 'code': None}}
------------------------------------------
Retry in 32s..
Round 3/3 to collecting reference responses.
 39%|███▊      | 191/496 [16:48:06<110:37:18, 1305.70s/it]Round 1/3 to collecting reference responses.
Round 2/3 to collecting reference responses.
Round 3/3 to collecting reference responses.
------------------------------------------
Model with Error: meta-llama/Llama-3-70b-chat-hf
{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 8193. Given: 4186 `inputs` tokens and 4096 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}
------------------------------------------
 39%|███▊      | 192/496 [16:53:02<84:41:21, 1002.90s/it] Round 1/3 to collecting reference responses.
